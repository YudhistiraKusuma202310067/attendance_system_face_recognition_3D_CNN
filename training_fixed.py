# # -*- coding: utf-8 -*-
# """Copy of FR-Training-Fixed.ipynb

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/1ivNOJaAdosm3EUrSOQEI4u3HBwlNKkgf
# """

# from google.colab import drive
# drive.mount('/content/drive')

# base_dir = '/content/drive/My Drive/Document/Documents Skripsi/2023-2024 Ganjil/Testing/Face-Library/'

import keras
import cv2
import matplotlib.pyplot as plt
import os
import itertools
import numpy as np
import matplotlib.pyplot as plt
from mtcnn import MTCNN

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

from keras.models import Sequential, Model
from keras.layers import Dense, Activation, Input
from keras.utils import to_categorical
from keras.layers import Conv2D, MaxPool2D, Flatten

# from google.colab import drive, files
# from google.colab.patches import cv2_imshow

# """1.1 read image & convert to grayscale"""

#ganti jd MTCNN
# def detect_face(img):
#     img = img[70:195,78:172]
#     img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
#     img = cv2.resize(img, (50, 50))
#     return img

def detect_face(img):
    detector = MTCNN()
    result = detector.detect_faces(img)
    if result:
        x, y, w, h = result[0]['box']
        face = img[y:y+h, x:x+w]
        face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)
        face = cv2.resize(face, (50, 50))
        return face
    else:
        return None

def print_progress(val, val_len, folder, bar_size=20):
    progr = "#"*round((val)*bar_size/val_len) + " "*round((val_len - (val))*bar_size/val_len)
    if val == 0:
        print("", end = "\n")
    else:
        print("[%s] (%d samples)\t label : %s \t\t" % (progr, val+1, folder), end="\r")

dataset_folder = "be_augmented_dataset_5/"

#print(os.listdir(dataset_folder))

names = []
images = []
for folder in os.listdir(dataset_folder):
    files = os.listdir(os.path.join(dataset_folder, folder))[:150]
    if len(files) < 20 :
        continue
    for i, name in enumerate(files):
        #print(name)
        if name.find(".jpg") > -1 :
            print(name)
            img = cv2.imread(os.path.join(dataset_folder + folder, name))
            img = detect_face(img) # detect face using mtcnn

            #gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            #face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
            #img = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)

            if img is not None :
                images.append(img)
                names.append(folder)
                print_progress(i, len(files), folder)

print("number of samples :", len(names))
#print(images[0])

# """1.1.A Image Augmentation"""

def img_augmentation(img):
    h, w = img.shape
    center = (w // 2, h // 2)
    M_rot_5 = cv2.getRotationMatrix2D(center, 5, 1.0)
    M_rot_neg_5 = cv2.getRotationMatrix2D(center, -5, 1.0)
    M_rot_10 = cv2.getRotationMatrix2D(center, 10, 1.0)
    M_rot_neg_10 = cv2.getRotationMatrix2D(center, -10, 1.0)
    M_trans_3 = np.float32([[1, 0, 3], [0, 1, 0]])
    M_trans_neg_3 = np.float32([[1, 0, -3], [0, 1, 0]])
    M_trans_6 = np.float32([[1, 0, 6], [0, 1, 0]])
    M_trans_neg_6 = np.float32([[1, 0, -6], [0, 1, 0]])
    M_trans_y3 = np.float32([[1, 0, 0], [0, 1, 3]])
    M_trans_neg_y3 = np.float32([[1, 0, 0], [0, 1, -3]])
    M_trans_y6 = np.float32([[1, 0, 0], [0, 1, 6]])
    M_trans_neg_y6 = np.float32([[1, 0, 0], [0, 1, -6]])

    imgs = []
    imgs.append(cv2.warpAffine(img, M_rot_5, (w, h), borderValue=(255,255,255)))
    imgs.append(cv2.warpAffine(img, M_rot_neg_5, (w, h), borderValue=(255,255,255)))
    imgs.append(cv2.warpAffine(img, M_rot_10, (w, h), borderValue=(255,255,255)))
    imgs.append(cv2.warpAffine(img, M_rot_neg_10, (w, h), borderValue=(255,255,255)))
    imgs.append(cv2.warpAffine(img, M_trans_3, (w, h), borderValue=(255,255,255)))
    imgs.append(cv2.warpAffine(img, M_trans_neg_3, (w, h), borderValue=(255,255,255)))
    imgs.append(cv2.warpAffine(img, M_trans_6, (w, h), borderValue=(255,255,255)))
    imgs.append(cv2.warpAffine(img, M_trans_neg_6, (w, h), borderValue=(255,255,255)))
    imgs.append(cv2.warpAffine(img, M_trans_y3, (w, h), borderValue=(255,255,255)))
    imgs.append(cv2.warpAffine(img, M_trans_neg_y3, (w, h), borderValue=(255,255,255)))
    imgs.append(cv2.warpAffine(img, M_trans_y6, (w, h), borderValue=(255,255,255)))
    imgs.append(cv2.warpAffine(img, M_trans_neg_y6, (w, h), borderValue=(255,255,255)))
    imgs.append(cv2.add(img, 10))
    imgs.append(cv2.add(img, 30))
    imgs.append(cv2.add(img, -10))
    imgs.append(cv2.add(img, -30))
    imgs.append(cv2.add(img, 15))
    imgs.append(cv2.add(img, 45))
    imgs.append(cv2.add(img, -15))
    imgs.append(cv2.add(img, -45))

    return imgs

# """Cuman testing abaikan"""

# plt.imshow(images[0], cmap="gray")

# #testing image augmented
# img_test = images[0]

# augmented_image_test = img_augmentation(img_test)

# plt.figure(figsize=(5,5))
# for i, img in enumerate(augmented_image_test):
#     plt.subplot(4,5,i+1)
#     plt.imshow(img, cmap="gray")
#     plt.axis('off')
# plt.subplots_adjust(wspace=0.05, hspace=0.05)
# plt.show()

# """Coba ke seluruh dataset untuk augmented


# """

augmented_images = []
augmented_names = []
for i, img in enumerate(images):
    try :
        augmented_images.extend(img_augmentation(img))
        augmented_names.extend([names[i]] * 20)
    except :
        print(i)

len(augmented_images), len(augmented_names)

images.extend(augmented_images)
names.extend(augmented_names)

# """Total seluruh dataset:"""

len(images), len(names)

unique, counts = np.unique(names, return_counts = True)

for item in zip(unique, counts):
    print(item)

def print_data(label_distr, label_name):
    plt.figure(figsize=(12,6))

    my_circle = plt.Circle( (0,0), 0.7, color='white')
    plt.pie(label_distr, labels=label_name, autopct='%1.1f%%')
    plt.gcf().gca().add_artist(my_circle)
    plt.show()

unique = np.unique(names)
label_distr = {i:names.count(i) for i in names}.values()
print_data(label_distr, unique)

# """Encoding Label & Categoricalization"""

le = LabelEncoder()

le.fit(names)

labels = le.classes_

name_vec = le.transform(names)

categorical_name_vec = to_categorical(name_vec)

print("number of class :", len(labels))
print(labels)

print(name_vec)
print(categorical_name_vec)

# """Split dataset for validation"""

x_train, x_test, y_train, y_test = train_test_split(np.array(images, dtype=np.float32),   # input data
                                                    np.array(categorical_name_vec),       # target/output data
                                                    test_size=0.15,
                                                    random_state=42)

print(x_train.shape, y_train.shape, x_test.shape,  y_test.shape)

# """ Reshape Data"""

x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)
x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)

x_train.shape, x_test.shape

# """Create CNN Model"""

def cnn_model(input_shape):
    model = Sequential()

    model.add(Conv2D(64,
                    (3,3),
                    padding="valid",
                    activation="relu",
                    input_shape=input_shape))
    model.add(Conv2D(64,
                    (3,3),
                    padding="valid",
                    activation="relu",
                    input_shape=input_shape))

    model.add(MaxPool2D(pool_size=(2, 2)))

    model.add(Conv2D(128,
                    (3,3),
                    padding="valid",
                    activation="relu"))
    model.add(Conv2D(128,
                    (3,3),
                    padding="valid",
                    activation="relu"))
    model.add(MaxPool2D(pool_size=(2, 2)))

    model.add(Flatten())

    model.add(Dense(128, activation="relu"))
    model.add(Dense(64, activation="relu"))
    model.add(Dense(len(labels)))  # equal to number of classes
    model.add(Activation("softmax"))

    model.summary()

    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics = ['accuracy'])

    return model

# """ Training CNN Model"""

input_shape = x_train[0].shape

EPOCHS = 10
BATCH_SIZE = 32

model = cnn_model(input_shape)

history = model.fit(x_train,
                    y_train,
                    epochs=EPOCHS,
                    batch_size=BATCH_SIZE,
                    shuffle=True,
                    validation_split=0.15   # 15% of train dataset will be used as validation set
                    )

def evaluate_model_(history):
    #print(history.history.keys())
    names = [['accuracy', 'val_accuracy'],
             ['loss', 'val_loss']]
    for name in names :
        fig1, ax_acc = plt.subplots()
        plt.plot(history.history[name[0]])
        plt.plot(history.history[name[1]])
        plt.xlabel('Epoch')
        plt.ylabel(name[0])
        plt.title('Model - ' + name[0])
        plt.legend(['Training', 'Validation'], loc='lower right')
        plt.grid()
        plt.show()

evaluate_model_(history)

#save the model

model.save("model/facerecognition-ep-8-training.h5")

# predict test data
y_pred=model.predict(x_test)

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    plt.figure(figsize=(8, 8))

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()

    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()

# Compute confusion matrix
cnf_matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))
np.set_printoptions(precision=2)


# Plot non-normalized confusion matrix
plot_confusion_matrix(cnf_matrix, classes=labels,normalize=False,
                      title='Confusion matrix')

print(classification_report(y_test.argmax(axis=1),
                            y_pred.argmax(axis=1),
                            target_names=labels))

# """**Testing**"""

from keras.models import load_model

def draw_ped(img, label, x0, y0, xt, yt, color=(255,127,0), text_color=(255,255,255)):

    (w, h), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
    cv2.rectangle(img,
                  (x0, y0 + baseline),
                  (max(xt, x0 + w), yt),
                  color,
                  2)
    cv2.rectangle(img,
                  (x0, y0 - h),
                  (x0 + w, y0 + baseline),
                  color,
                  -1)
    cv2.putText(img,
                label,
                (x0, y0),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.5,
                text_color,
                1,
                cv2.LINE_AA)
    return img

model_face_recog = "model/facerecognition-ep-8-training.h5";
testing_path = "dataset_testing/"
testing_path_group = "dataset_testing_group/"

# # --------- load Haar Cascade model -------------
# face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades+'/haarcascade_frontalface_default.xml')

# # --------- load Keras CNN model -------------
# model = load_model(model_face_recog)
# print("[INFO] finish load model...")

# def face_recognition(img):
#     frame = cv2.imread(img)

#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
#     faces = face_cascade.detectMultiScale(gray, 1.1, 5)

#     for (x, y, w, h) in faces:
#         face_img = gray[y:y+h, x:x+w]
#         face_img = cv2.resize(face_img, (50, 50))
#         face_img = face_img.reshape(1, 50, 50, 1)

#         result = model.predict(face_img)
#         idx = result.argmax(axis=1)
#         confidence = result.max(axis=1) * 100
#         predictes_as = "N/A"
#         for i in idx:
#           if(confidence > 80):
#             predictes_as = "%s (%.2f %%)" % (labels[i], confidence)
#           else:
#             predictes_as = "N/A"

#           print("Predicted as :",labels[i], "Persentase:", confidence)

#         label_text = predictes_as
#         frame = draw_ped(frame, label_text, x, y, x + w, y + h, color=(255, 000, 000), text_color=(50, 50, 50))

# --------- load MTCNN model -------------
detector = MTCNN()

# --------- load Keras CNN model -------------
model = load_model(model_face_recog)
print("[INFO] finish load model...")

# dataset_dir = 'be_augmented_dataset_5'
# labels = [folder for folder in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, folder))]

def face_recognition(img):
    frame = cv2.imread(img)

    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    
    # Detect faces using MTCNN
    faces = detector.detect_faces(frame)

    for face in faces:
        x, y, w, h = face['box']
        
        face_img = gray[y:y+h, x:x+w]
        face_img = cv2.resize(face_img, (50, 50))
        face_img = face_img.reshape(1, 50, 50, 1)

        result = model.predict(face_img)
        idx = result.argmax(axis=1)
        confidence = result.max(axis=1) * 100
        predicted_as = "N/A"
        for i in idx:
            if confidence > 80:
                predicted_as = "%s (%.2f %%)" % (labels[i], confidence)
            else:
                predicted_as = "N/A"

            print("Predicted as :", labels[i], "Persentase:", confidence)

    #     label_text = predicted_as
    #     frame = draw_ped(frame, label_text, x, y, x + w, y + h, color=(255, 0, 0), text_color=(50, 50, 50))

    # plt.figure(figsize=(15,10))
    # plt.imshow(frame)
    # plt.title(f"Images : "+ img)

        # Gambarkan kotak di sekitar wajah pada gambar asli
        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)

        # Tampilkan prediksi label dan akurasi pada gambar asli
        font = cv2.FONT_HERSHEY_SIMPLEX
        org = (x, y - 10)
        fontScale = 1
        color = (255, 0, 0)
        thickness = 2
        frame = cv2.putText(frame, predicted_as, org, font, fontScale, color, thickness, cv2.LINE_AA)

    # Tampilkan gambar asli dengan kotak di sekitar wajah dan label prediksi
    plt.figure(figsize=(15,10))
    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    plt.axis('off')
    plt.show()

# Single Testing
for filename in os.listdir(testing_path):
    image_path = os.path.join(testing_path, filename)
    face_recognition(image_path)

# Group Testing
# for filename in os.listdir(testing_path_group):
#     image_path = os.path.join(testing_path_group, filename)
#     face_recognition(image_path)
    

# face_recognition(testing_path + "2.jpeg")

# for x in range(6):
#     img_ = image_path_single + str(x+1) + ".jpeg"
#     face_recognition(img_)